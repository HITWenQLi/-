{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 迁移学习 Code\n",
    "# 2020年2月11日 WenQLi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 一、迁移学习模型代码实现"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "def finetune(dataloaders, model_name, sets, num_epochs, num_gpus, lr, momentum, lr_step, lr_epochs, verbose=True):\n",
    "    #Class adaptation\n",
    "    num_class = len(dataloaders[sets[0]].dataset.class_to_idx)\n",
    "    model_ft = models.__dict__[model_name](pretrained=True)\n",
    "    num_ftrs = model_ft.fc.in_features\n",
    "    model_ft.fc = nn.Linear(num_ftrs, num_class)\n",
    "    \n",
    "    #gpus\n",
    "    if num_gpus > 1: \n",
    "        model_ft = nn.DataParallel(model_ft)\n",
    "    model_ft = model_ft.cuda()\n",
    "    \n",
    "    #loss\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "    # All parameters are being optimized\n",
    "    optimizer = SGD(model_ft.parameters(), lr=lr, momentum=momentum)\n",
    "\n",
    "    # Decay LR by a factor of lr_step every lr_epochs epochs\n",
    "    exp_lr_scheduler = lr_scheduler.StepLR(optimizer, step_size=lr_epochs, gamma=lr_step)\n",
    "    model_ft = train_model(dataloaders, model_ft, sets, criterion, optimizer, exp_lr_scheduler, \n",
    "                           num_epochs=num_epochs, verbose=verbose)\n",
    "    return model_ft"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def freeze_and_train(dataloaders, model_name, sets, num_epochs, num_gpus, lr, momentum, lr_step, lr_epochs, verbose=True):\n",
    "    #Class adaptation\n",
    "    num_class = len(dataloaders[sets[0]].dataset.class_to_idx)\n",
    "    model_conv = models.__dict__[model_name](pretrained=True)\n",
    "    for param in model_conv.parameters(): #params have requires_grad=True by default\n",
    "        param.requires_grad = False\n",
    "    num_ftrs = model_conv.fc.in_features\n",
    "    model_conv.fc = nn.Linear(num_ftrs, num_class)\n",
    "    \n",
    "    #gpus\n",
    "    if num_gpus > 1: \n",
    "        model_conv = nn.DataParallel(model_conv)\n",
    "    model_conv = model_conv.cuda()\n",
    "    \n",
    "    #loss\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "    # Only parameters of final layer are being optimized \n",
    "    if num_gpus > 1:\n",
    "        params = model_conv.module.fc.parameters()\n",
    "    else:\n",
    "        params = model_conv.fc.parameters()\n",
    "    optimizer = SGD(params, lr=lr, momentum=momentum)\n",
    "\n",
    "    # Decay LR by a factor of lr_step every lr_epochs epochs\n",
    "    exp_lr_scheduler = lr_scheduler.StepLR(optimizer, step_size=lr_epochs, gamma=lr_step)\n",
    "    model_conv = train_model(dataloaders, model_conv, sets, criterion, optimizer, exp_lr_scheduler, \n",
    "                             num_epochs=num_epochs, verbose=verbose)\n",
    "    return model_conv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "根据新任务的识别种类数量，finetune模型在原模型的最后一层上添加全连接层，实现迁移学习任务。\n",
    "freeze_and_train模型 通过param.requires_grad = False语句冻结相关的参数，实现相关模型的冻结。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(dataloaders, model, sets, criterion, optimizer, scheduler, num_epochs=25, verbose=False):\n",
    "    \"\"\"Train a pytorch model\"\"\"\n",
    "    since = time.time()\n",
    "    dataset_sizes = {x: len(dataloaders[x].dataset) for x in sets}\n",
    "    best_model_wts = model.state_dict()\n",
    "    best_acc = 0.0\n",
    "    num_classes = len(dataloaders[sets[0]].dataset.classes)\n",
    "    confusion_matrix = tnt.meter.ConfusionMeter(num_classes, normalized=False)\n",
    "    metrics = {'train_acc':[],'val_acc':[],'train_loss':[],'val_loss':[], 'cm':[]}\n",
    "    for epoch in range(num_epochs):\n",
    "        if verbose:\n",
    "            print('\\nEpoch {}/{}'.format(epoch + 1, num_epochs))\n",
    "            print('-' * 10)\n",
    "\n",
    "        # Each epoch has a training and validation phase\n",
    "        for phase in sets:\n",
    "            if phase == 'train':\n",
    "                scheduler.step()\n",
    "                model.train(True)  # Set model to training mode\n",
    "            else:\n",
    "                model.train(False)  # Set model to evaluate mode\n",
    "                confusion_matrix.reset()\n",
    "\n",
    "            running_loss = 0.0\n",
    "            running_corrects = 0\n",
    "\n",
    "            # Iterate over data.\n",
    "            for data in dataloaders[phase]:\n",
    "                # get the inputs\n",
    "                inputs, labels = data\n",
    "\n",
    "                # wrap them in Variable\n",
    "                inputs = Variable(inputs.cuda())\n",
    "                labels = Variable(labels.cuda())\n",
    "\n",
    "\n",
    "                # zero the parameter gradients\n",
    "                optimizer.zero_grad()\n",
    "\n",
    "                # forward\n",
    "                outputs = model(inputs)\n",
    "                _, preds = torch.max(outputs.data, 1)\n",
    "                loss = criterion(outputs, labels)\n",
    "\n",
    "                # backward + optimize only if in training phase\n",
    "                if phase == 'train':\n",
    "                    loss.backward()\n",
    "                    optimizer.step()\n",
    "                else:\n",
    "                    confusion_matrix.add(outputs.data, labels.data)\n",
    "                    \n",
    "                # statistics\n",
    "                running_loss += loss.data[0]\n",
    "                running_corrects += torch.sum(preds == labels.data)\n",
    "\n",
    "            #metrics\n",
    "            epoch_loss = running_loss / dataset_sizes[phase]\n",
    "            epoch_acc = running_corrects / dataset_sizes[phase]\n",
    "            if verbose: print('{} Loss: {:.4f} Acc: {:.4f}'.format(phase, epoch_loss, epoch_acc))\n",
    "            if phase == 'train':\n",
    "                metrics['train_acc'].append(epoch_acc)\n",
    "                metrics['train_loss'].append(epoch_loss)\n",
    "            else:\n",
    "                metrics['val_acc'].append(epoch_acc)\n",
    "                metrics['val_loss'].append(epoch_loss)\n",
    "                cm = confusion_matrix.value().copy()\n",
    "                metrics['cm'].append(cm)\n",
    "                \n",
    "            # deep copy the model\n",
    "            if phase == 'val' and epoch_acc > best_acc:\n",
    "                best_acc = epoch_acc\n",
    "                best_model_wts = copy.deepcopy(model.state_dict())\n",
    "\n",
    "    time_elapsed = time.time() - since\n",
    "    if verbose:\n",
    "        print('Training complete in {:.0f}m {:.0f}s'.format(time_elapsed // 60, time_elapsed % 60))\n",
    "        print('Best val Acc: {:4f}'.format(best_acc))\n",
    "\n",
    "    # load best model weights\n",
    "    model.load_state_dict(best_model_wts)\n",
    "    return model, metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 二、开始执行"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_NAME = 'resnet18'\n",
    "BATCH_SIZE = 64\n",
    "SETS = ['train', 'val']\n",
    "NUM_GPUS = 4\n",
    "EPOCHS = 15\n",
    "LR = 0.001\n",
    "LR_STEP = 0.1\n",
    "LR_EPOCHS = 10\n",
    "MOMENTUM = 0.9"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "初始化参数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'create_dataset' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-5-55acac548499>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mdata_hymenoptera\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcreate_dataset\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mHYMENOPTERA_ROOT\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mBATCH_SIZE\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'create_dataset' is not defined"
     ]
    }
   ],
   "source": [
    "data_hymenoptera = create_dataset(HYMENOPTERA_ROOT, batch_size=BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'plot_pytorch_data_stream' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-6-9d0a45985691>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mplot_pytorch_data_stream\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata_hymenoptera\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'train'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmax_images\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m4\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'plot_pytorch_data_stream' is not defined"
     ]
    }
   ],
   "source": [
    "plot_pytorch_data_stream(data_hymenoptera['train'], max_images=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_pytorch_data_stream(dataobject, max_images=8, title=True):\n",
    "    \"\"\"Plot a batch of images\"\"\"\n",
    "    inputs, classes = next(iter(dataobject))  \n",
    "    if max_images > dataobject.batch_size:\n",
    "        max_images = dataobject.batch_size\n",
    "        print(\"Plotting only {} images, which is the batch size\".format(max_images))\n",
    "    inputs = inputs[:max_images,:,:,:]\n",
    "    classes = classes[:max_images]\n",
    "    out = torchvision.utils.make_grid(inputs)\n",
    "    inp = out.numpy().transpose((1, 2, 0))\n",
    "    mean = np.array([0.485, 0.456, 0.406])\n",
    "    std = np.array([0.229, 0.224, 0.225])\n",
    "    inp = std * inp + mean\n",
    "    inp = np.clip(inp, 0, 1)\n",
    "    plt.imshow(inp)\n",
    "    plt.axis('off')\n",
    "    if title:\n",
    "        class_names = dataobject.dataset.classes\n",
    "        names = [class_names[x] for x in classes]\n",
    "        plt.title(names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_dataset(data_dir, batch_size=32, sets=['train', 'val'], verbose=False):\n",
    "    \"\"\"Create a dataset object given the path. On data_dir there should be a train and validation folder\n",
    "    and in each of them there should be the folders containing the data. One folder for each class\n",
    "    \"\"\"\n",
    "    data_transforms = {\n",
    "        'train': transforms.Compose([\n",
    "            transforms.Scale(256),\n",
    "            transforms.RandomSizedCrop(224),\n",
    "            transforms.RandomHorizontalFlip(),\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "        ]),\n",
    "        'val': transforms.Compose([\n",
    "            transforms.Scale(256),\n",
    "            transforms.CenterCrop(224),\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "        ]),\n",
    "    }\n",
    "    image_datasets = {x: datasets.ImageFolder(os.path.join(data_dir, x), data_transforms[x]) for x in sets}\n",
    "    dataloaders = {x: DataLoader(image_datasets[x], \n",
    "                                 batch_size=batch_size, \n",
    "                                 shuffle=True, \n",
    "                                 num_workers=get_number_processors()) \n",
    "                   for x in sets}\n",
    "\n",
    "    if verbose:\n",
    "        dataset_sizes = {x: len(image_datasets[x]) for x in sets}\n",
    "        class_names = dataloaders[sets[0]].dataset.class_to_idx\n",
    "        print(\"There are {} clases in the dataset: {}\".format(len(class_names), format_dictionary(class_names)))\n",
    "        print(\"Sets sizes: \", format_dictionary(dataset_sizes))\n",
    "        for x in sets:   \n",
    "            c = Counter(item[1] for item in image_datasets[x])\n",
    "            c = dict(c)\n",
    "            print(\"Number of items in set {}: {}\".format(x, c))\n",
    "    return dataloaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-9-07623b651fa0>, line 3)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  File \u001b[1;32m\"<ipython-input-9-07623b651fa0>\"\u001b[1;36m, line \u001b[1;32m3\u001b[0m\n\u001b[1;33m    将输入的`PIL.Image`重新改变大小成给定的`size`，`size`是最小边的边长。举个例子，如果原图的`height>width`,那么改变大小后的图片大小是`(size*height/width, size)`。\u001b[0m\n\u001b[1;37m        ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "### class torchvision.transforms.Scale(size, interpolation=2)\n",
    "\n",
    "将输入的`PIL.Image`重新改变大小成给定的`size`，`size`是最小边的边长。举个例子，如果原图的`height>width`,那么改变大小后的图片大小是`(size*height/width, size)`。\n",
    "**用例:**\n",
    "```python\n",
    "from torchvision import transforms\n",
    "from PIL import Image\n",
    "crop = transforms.Scale(12)\n",
    "img = Image.open('test.jpg')\n",
    "\n",
    "print(type(img))\n",
    "print(img.size)\n",
    "\n",
    "croped_img=crop(img)\n",
    "print(type(croped_img))\n",
    "print(croped_img.size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
